% Это основная команда, с которой начинается любой \LaTeX-файл. Она отвечает за тип документа, с которым связаны основные правил оформления текста.
\documentclass{article}

% Здесь идет преамбула документа, тут пишутся команды, которые настраивают LaTeX окружение, подключаете внешние пакеты, определяете свои команды и окружения. В данном случае я это делаю в отдельных файлах, а тут подключаю эти файлы.

% Здесь я подключаю разные стилевые пакеты. Например возможности набирать особые символы или возможность компилировать русский текст. Подробное описание внутри.
\usepackage{packages}

% Здесь я определяю разные окружения, например, теоремы, определения, замечания и так далее. У этих окружений разные стили оформления, кроме того, эти окружения могут быть нумерованными или нет. Все подробно объяснено внутри.
\usepackage{environments}

% Здесь я определяю разные команды, которых нет в LaTeX, но мне нужны, например, команда \tr для обозначения следа матрицы. Или я переопределяю LaTeX команды, которые работают не так, как мне хотелось бы. Типичный пример мнимая и вещественная часть комплексного числа \Im, \Re. В оригинале они выглядят не так, как мы привыкли. Кроме того, \Im еще используется и для обозначения образа линейного отображения. Подробнее описано внутри.
\usepackage{commands}

% Потребуется для вставки картинки подписи
% \usepackage{graphicx}

% Пакет для титульника проекта
\usepackage{titlepage}

% Здесь задаем параметры титульной страницы
%\setUDK{192.168.1.1}
% Выбрать одно из двух
%\setToResearch
\setToProgram

\setTitle{Нейросети с нуля}

% Выбрать одно из трех:
% КТ1 -- \setStageOne
% КТ2 -- \setStageTwo
% Финальная версия -- \setStageFinal
\setStageOne
%\setStageTwo
%\setStageFinal

\setGroup{221}
% Сюда можно воткнуть картинку подписи с помощью \includegraphics[scale=0.2]{<имя файла>}
% (scale подбирается индивидуально для конкретной картинки)
\setStudentSgn{}
\setStudent{Иван Игоревич Плешков}
\setStudentDate{06.02.2024}
\setAdvisor{Дмитрий Витальевич Трушин}
\setAdvisorTitle{доцент, к.ф.-м.н.}
\setAdvisorAffiliation{ФКН НИУ ВШЭ}
\setAdvisorDate{06.02}
\setGrade{}
% Сюда можно воткнуть картинку подписи с помощью \includegraphics[scale=0.2]{<имя файла>}
% (scale подбирается индивидуально для конкретной картинки)
\setAdvisorSgn{}
\setYear{2024}


% С этого момента начинается текст документа
\begin{document}

% Эта команда создает титульную страницу
\makeTitlePage

% Здесь будет автоматически генерироваться содержание документа
\tableofcontents

\newpage
% Данное окружение оформляет аннотацию: краткое описание текста выделенным абзацем после заголовка
\begin{abstract}
В рамках данного курсового проекта рассматривается разработка и реализация нейронной сети на языке программирования C++ для задачи распознавания рукописных цифр. Проект направлен на глубокое изучение теоретических основ нейронных сетей, а также практическое применение полученных знаний для создания работающей модели. \newline \textit{Ключевые слова: машинное обучение, нейронные сети, распознавание цифр, обратное распространение ошибки, градиентный спуск.}
\end{abstract}


\section{Введение}
Нейронные сети представляют собой мощные вычислительные системы, вдохновленные структурой и принципами работы человеческого мозга. Благодаря их способности к обучению нейронные сети находят широкое применение в различных областях, от распознавания образов, звука и текста до прогнозирования временных рядов и автономного управления транспортными средствами. Нейронные сети состоят из взаимосвязанных слоев "нейронов" –  вычислительных единиц, которые обрабатывают входные данные и передают результаты дальше по сети. Каждый отдельный нейрон можно представлять как функцию, принимающую на вход несколько значений и выдающую один результат. Нейронные сети работают эффективно, потому что они умеют обучаться, используя различные алгоритмы обучения. Одним из основных методов обучения нейронных сетей является метод обратного распространения ошибки. Во время обучения этот алгоритм работает в обратном направлении, сравнивая выходные данные каждого слоя с ожидаемыми результатами, оценивая ошибки и корректируя параметры слоев, чтобы улучшить работу сети. Для людей, не знакомых с принципами работы нейронных сетей, структура слоев нейронов и метод обратного распространения ошибки могут представлять собой своеобразный "черный ящик". 

Цель данного курсового проекта – изучить устройство нейронных сетей, методов их обучения и разработать собственную нейронную сеть с нуля на языке программирования C++ для распознавания рукописных цифр. Это не просто техническая задача реализации архитектуры нейросети, но и глубокое погружение в основы теории нейронных сетей, изучение их математической модели, алгоритмов обучения и принципов работы. Проект предполагает не только теоретический анализ существующих методов, но и практическое применение этих знаний для создания работоспособной модели, способной эффективно распознавать рукописные цифры на основе датасета MNIST.

Основными задачами проекта являются:
\begin{itemize}
    \item Изучение теоретических основ нейронных сетей, включая их структуру, принципы обучения и алгоритмы оптимизации.
    \item Разработка и реализация компонентов нейронной сети, включая слои, функций активаций, функции потерь и общую архитектуру сети.
    \item Обучение разработанной модели на датасете MNIST и оценка её способности корректно распознавать рукописные цифры.
    \item Проведение различных тестов, сравнение и анализ разных конфигурации нейронной сети.
\end{itemize}

\section{Функциональные требования}
Данный проект представляет собой программу для распознавания рукописных цифр, разрабатываемую на языке программирования C++17. Для реализации вычислений, связанных с линейной алгеброй, используется библиотека Eigen, что позволяет эффективно работать с матрицами и векторами, необходимыми для обработки данных и обучения сети.
Контроль версий осуществляется с помощью системы git. Для облачного хранения и публикации исходного кода используется платформа GitHub. 
На данный момент написаны ключевые классы для работы нейронной сети. Далее описаны классы, которые были реализованы на данный момент в рамках проекта, и их ключевые функциональные аспекты.

\subsection{Класс \texttt{NeuralNetwork}}
Класс, интегрирующий все необходимые компоненты в единую структуру нейронной сети. Он отвечает за:
\begin{itemize}
    \item Сборку сети из последовательности слоев с заданными параметрами.
    \item Выполнение прямого распространения входного сигнала через слои сети для получения предсказания.
    \item Выполнение обратного распространения ошибки для обновления весов сети на основе заданной функции потерь и скорости обучения.
    \item Управление процессом обучения сети, включая подачу обучающих примеров и адаптацию параметров сети для минимизации ошибки.
\end{itemize}

\subsection{Класс \texttt{Layer}}
Этот класс представляет собой один слой нейронной сети, состоящий из нейронов. Основные функции класса включают:
\begin{itemize}
    \item Инициализацию весов и смещений нейронов слоя с использованием нормального распределения для начальных значений.
    \item Выполнение прямого распространения, принимая входные данные и применяя активационную функцию для получения выходных значений слоя.
    \item Выполнение обратного распространения, обновляя веса и смещения на основе градиента потерь.
    \item Обновление весов в соответствии с полученным градиентом и заданной скоростью обучения.
\end{itemize}

\subsection{Класс \texttt{ActivationFunction}}
Абстрактный базовый класс для активационных функций, определяющий интерфейс для вычисления активационной функции и её производной. Каждая конкретная функция активации должна реализовать следующие методы:
\begin{itemize}
    \item \texttt{compute}: Вычисляет значение функции активации для входа \texttt{x}.
    \item \texttt{computeDerivative}: Вычисляет производную функции активации в точке \texttt{x}.
\end{itemize}

\subsection{Класс \texttt{SigmoidActivation}}
Реализация класса \texttt{ActivationFunction}, представляющая сигмоидальную функцию активации. Сигмоидальная функция характеризуется гладким переходом выходного сигнала от 0 к 1, что делает её подходящей для использования в последнем слое нейронной сети при решении задач классификации.

\subsection{Класс \texttt{LossFunction}}
Абстрактный класс, описывающий интерфейс для функций потерь. Функция потерь необходима для оценки эффективности сети на этапе обучения. Ключевые методы класса включают:
\begin{itemize}
    \item \texttt{computeLoss}: Вычисляет значение функции потерь, сравнивая предсказанные значения с фактическими.
    \item \texttt{computeDerivativeLoss}: Вычисляет градиент функции потерь, необходимый для обратного распространения ошибки.
\end{itemize}

\subsection{Класс \texttt{MeanSquaredError}}
Конкретная реализация \texttt{LossFunction}, представляющая функцию среднеквадратичной ошибки (MSE). MSE является стандартной метрикой для оценки производительности сети в задачах регрессии и классификации, где необходимо минимизировать разницу между предсказанными и реальными значениями.

\section{Взаимодействие классов в архитектуре нейронной сети}

В архитектуре разработанной нейронной сети ключевыми элементами являются классы \texttt{NeuralNetwork},  \texttt{Layer}, \texttt{ActivationFunction}, включая его конкретные реализации, и классы функций потерь. Взаимодействие этих классов обеспечивает полный цикл работы нейросети, от прямого распространения сигнала до обучения сети с использованием обратного распространения ошибки.

\textbf{Сборка и инициализация сети.} Класс \texttt{NeuralNetwork} отвечает за создание и инициализацию нейронной сети, агрегируя в себе множество слоёв (\texttt{Layer}). При инициализации экземпляра \texttt{NeuralNetwork}, он последовательно создаёт слои сети, каждый из которых инициализируется с заданным количеством нейронов и выбранной функцией активации, например, \texttt{SigmoidActivation}.

\textbf{Прямое распространение.} В процессе прямого распространения, класс \texttt{Layer} принимает входные данные, выполняет матричное умножение входных данных на матрицу весов слоя, добавляет смещение и применяет функцию активации. Результатом является выходной вектор слоя, который передаётся как вход следующему слою в сети.

\textbf{Обратное распространение и обновление весов.} После получения значения функции потерь, например, с использованием \texttt{MeanSquaredError}, начинается процесс обратного распространения ошибки. Каждый слой сети (\texttt{Layer}) обновляет свои веса и смещения в соответствии с градиентом функции потерь, что позволяет минимизировать ошибку на выходе сети. Обновление параметров слоя осуществляется на основе градиентов, полученных в ходе обратного распространения, и скорости обучения.

\textbf{Функции активации и потерь.} Функция активации, на данный момент реализованная в классе \newline \texttt{SigmoidActivation}, применяется к каждому нейрону слоя для внесения нелинейности в процесс обработки данных сетью. Функции потерь, на данный момент реализована \texttt{MeanSquaredError}, оценивают разницу между предсказаниями сети и реальными данными, что критически важно для процесса обучения.

\section{Нефункциональные требования}
\begin{itemize}
    \item C++17
    \item Библиотека Eigen для работы с линейной алгеброй
    \item Система контроля версий Git
    \item GitHub – платформа для хранения и управления программным обеспечением с использованием системы контроля версий Git. 
\end{itemize}

\section{Обзор литературы}

Для реализации проекта была изучена документация языка С++17\cite{cppreference}. Также для написания более качественного кода была прочитаны книга "C++ Primer Plus"\cite{pratac++}. Для работы с линейной алгеброй была изучена документация библиотеки Eigen\cite{eigen}, а также документация системы контроля версий Git\cite{git}. Теория, необходимая для реализации нейронной сети, была изучена в книгах "Математические основы машинного обучения и прогнозирования"\cite{math_ml} и "Neural Networks and Deep Learning"\cite{nnndl}, а также в курсе Евгения Соколова "Машинное обучение на ФКН ВШЭ"\cite{ml_sokolov}. Для изучения различных алгоритмов градиентного спуска была изучена статья "An overview of gradient descent optimization algorithms"\cite{optimization_algorithms}, а также две статьи про алгоритм оптимизации Adam\cite{adam} и алгоритм SAG\cite{stochastic}.

\section{План дальнейшей работы}

В рамках дальнейшей работы над проектом запланированы следующие ключевые этапы работы:

\begin{enumerate}
    \item Доработка и добавление классов необходимых для полноценной работы нейронной сети, включая слои с различными типами активационных функций и методы оптимизации для обучения сети.

    \item Написание тестов на работоспособность кода для обеспечения надежности и стабильности работы всех компонентов нейросети. Тестирование позволит своевременно выявлять ошибки и недоработки в логике и алгоритмах.
    
    \item Прохождение код ревью от научного руководителя для выявления потенциальных ошибок и улучшения качества кода. Это поможет обеспечить соответствие кода лучшим практикам разработки и улучшить его читаемость и поддерживаемость.
    
    \item Доработка кода после прохождения код ревью, включая исправление выявленных недочетов и реализацию рекомендованных улучшений.
    
    \item Обучение сети на датасете MNIST и оценка её эффективности работы. В этом этапе будет произведена настройка гиперпараметров сети и выбор наиболее эффективной архитектуры для задачи распознавания рукописных цифр.
    
    \item Проведение различных экспериментов с использованием различной конфигурации нейронной сети, включая эксперименты с разными функциями потерь и функциями активаций, для выявления оптимальных параметров сети и улучшения качества распознавания.
    
    \item Анализ результатов экспериментов для определения наилучших практик и подходов в построении и обучении нейросетей на примере задачи распознавания рукописных цифр.
    
    \item Оптимизация производительности сети, включая исследование и применение техник уменьшения времени обучения и увеличения скорости предсказания.
\end{enumerate}

\newpage
% Здесь автоматически генерируется библиография. Первая команда задает стиль оформления библиографии, а вторая указывает на имя файла с расширением bib, в котором находится информация об источниках.
% \bibliographystyle{plainurl}
\bibliographystyle{unsrt}
\bibliography{bibl}


% Здесь текст документа заканчивается
\end{document}
% Начиная с этого момента весь текст LaTeX игнорирует, можете вставлять любую абракадабру.
